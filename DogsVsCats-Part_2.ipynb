{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Dogs Vs Cats classification.\n",
    "* This is Part 2\n",
    "* Using a pre-trained convent CNN\n",
    "\n",
    "* part 1 and details: https://github.com/DevHwary/CNN-CatsVsDogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data is prepared, only get the directories of the data\n",
    "base_dir = '/home/hwary/Deep Learning With Python/5-DogsVsCats/cats_and_dogs_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "\n",
    "# Data Augmentation for only the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Instantiating the VGG16 convolutional base\n",
    "* By using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Fine-tuning\n",
    "Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tune the last three convolutional layers, which means all layers up to block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and block5_conv3 should be trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing all layers up to a specific one\n",
    "\n",
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Adding a densely connected classifier on top of the convolutional base\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))      \n",
    "model.add(layers.Dropout(0.5))          # the dropout layer fighting the overfitting \n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 803s 8s/step - loss: 0.6465 - acc: 0.6235 - val_loss: 0.4321 - val_acc: 0.8150\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 807s 8s/step - loss: 0.4754 - acc: 0.7730 - val_loss: 0.2960 - val_acc: 0.8710\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 806s 8s/step - loss: 0.3918 - acc: 0.8150 - val_loss: 0.2400 - val_acc: 0.8980\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 803s 8s/step - loss: 0.3485 - acc: 0.8440 - val_loss: 0.3006 - val_acc: 0.8660\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 804s 8s/step - loss: 0.3381 - acc: 0.8590 - val_loss: 0.2114 - val_acc: 0.9100\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 808s 8s/step - loss: 0.3098 - acc: 0.8640 - val_loss: 0.1935 - val_acc: 0.9140\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 806s 8s/step - loss: 0.2687 - acc: 0.8875 - val_loss: 0.1981 - val_acc: 0.9110\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 805s 8s/step - loss: 0.2524 - acc: 0.8850 - val_loss: 0.1789 - val_acc: 0.9200\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 804s 8s/step - loss: 0.2492 - acc: 0.8990 - val_loss: 0.1759 - val_acc: 0.9240\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 804s 8s/step - loss: 0.2400 - acc: 0.9050 - val_loss: 0.1737 - val_acc: 0.9280\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 809s 8s/step - loss: 0.2357 - acc: 0.9060 - val_loss: 0.1837 - val_acc: 0.9210\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 805s 8s/step - loss: 0.2195 - acc: 0.9040 - val_loss: 0.1808 - val_acc: 0.9230\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 805s 8s/step - loss: 0.2108 - acc: 0.9030 - val_loss: 0.1668 - val_acc: 0.9360\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 805s 8s/step - loss: 0.2119 - acc: 0.9070 - val_loss: 0.1640 - val_acc: 0.9350\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 804s 8s/step - loss: 0.1976 - acc: 0.9205 - val_loss: 0.1716 - val_acc: 0.9230\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 802s 8s/step - loss: 0.1946 - acc: 0.9240 - val_loss: 0.1641 - val_acc: 0.9310\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 800s 8s/step - loss: 0.1917 - acc: 0.9230 - val_loss: 0.1627 - val_acc: 0.9350\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 802s 8s/step - loss: 0.1858 - acc: 0.9225 - val_loss: 0.1595 - val_acc: 0.9320\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 800s 8s/step - loss: 0.1712 - acc: 0.9290 - val_loss: 0.1653 - val_acc: 0.9330\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 801s 8s/step - loss: 0.1632 - acc: 0.9370 - val_loss: 0.2008 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model number 2\n",
    "\n",
    "model.save('cats_and_dogs_small_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying curves of loss and accuracy during training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part Two is finished\n",
    "* Acc: 94%\n",
    "* Loss: 16%\n",
    "* Val_acc: 92%\n",
    "* Val_loss: 20%\n",
    "\n",
    "we hade fighted the overfitting and tuned the model well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if we make epochs=100 not only 20, we would reach accuracy equal 97%. But i have no GPU right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "('test accuracy: ', 0.9229999935626984)\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    test_dir,\n",
    "                    target_size=(150, 150),\n",
    "                    batch_size=20,\n",
    "                    class_mode='binary')\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracy would reach 97% too if we have finished the 100 epochs.\n",
    "it is Great! Woa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In part 3: Visualizing what convents 'CNN' learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
